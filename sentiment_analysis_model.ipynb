{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitpan/datasciencecoursera/blob/master/sentiment_analysis_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnnwupedFEIV"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "Let's use the sentiment analysis model you created to train a model using a dataset of Amazon product reviews, where we have the corresponding sentiment as a number between -1 and 1 (completely negative to completely positive).\n",
        "\n",
        "Note: Before running anything, go to Runtime -> Change Runtime Type -> T4 GPU to speed things up.\n",
        "\n",
        "First, we'll download the raw text file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/gptandchill/sentiment-analysis\n",
        "%cd sentiment-analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQEeXqFNr8zN",
        "outputId": "087d61e9-ee3a-4347-baf6-346b1b9c0641"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sentiment-analysis'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (6/6), 116.33 KiB | 838.00 KiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "/content/sentiment-analysis/sentiment-analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the dataset, let's convert it into PyTorch tensors. You can just run and ignore the details of parsing the text file."
      ],
      "metadata": {
        "id": "YX0EIM55z_cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "list_of_strings = []\n",
        "list_of_labels = []\n",
        "\n",
        "import csv\n",
        "with open('EcoPreprocessed.csv') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "      list_of_strings.append(row[1])\n",
        "      list_of_labels.append(float(row[2]))\n"
      ],
      "metadata": {
        "id": "eTFmS9HLsRs9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the actual tensors, we can borrow the solution code from the \"NLP Intro\" problem."
      ],
      "metadata": {
        "id": "4bNwK6Yn1U-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(list_of_strings):\n",
        "\n",
        "    # First let's get the total set of words\n",
        "    words = set()\n",
        "    for sentence in list_of_strings:\n",
        "        for word in sentence.split():\n",
        "            words.add(word)\n",
        "\n",
        "    vocab_size = len(words)\n",
        "\n",
        "    # Now let's build a mapping\n",
        "    sorted_list = sorted(list(words))\n",
        "    word_to_int = {}\n",
        "    for i, c in enumerate(sorted_list):\n",
        "        word_to_int[c] = i + 1\n",
        "\n",
        "    # Write encode() which is used to build the dataset\n",
        "\n",
        "    def encode(sentence):\n",
        "        integers = []\n",
        "        for word in sentence.split():\n",
        "            integers.append(word_to_int[word])\n",
        "        return integers\n",
        "\n",
        "    var_len_tensors = []\n",
        "    for sentence in list_of_strings:\n",
        "        var_len_tensors.append(torch.tensor(encode(sentence)))\n",
        "\n",
        "    return vocab_size + 1, nn.utils.rnn.pad_sequence(var_len_tensors, batch_first = True), word_to_int"
      ],
      "metadata": {
        "id": "-Jv1RkGf1JNV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, training_dataset, word_to_int = get_dataset(list_of_strings)\n",
        "print(\"list_of_labels\",torch.tensor(list_of_labels).shape)\n",
        "\n",
        "training_labels = torch.unsqueeze(torch.tensor(list_of_labels), dim = -1)\n",
        "print(\"training_labels\",training_labels.shape)"
      ],
      "metadata": {
        "id": "P2d-Zrfk1tK1",
        "outputId": "d32f3c84-977b-4700-d9e9-dc10a9bbf525",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list_of_labels torch.Size([4084])\n",
            "training_labels torch.Size([4084, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the dataset is in, let's use the model you wrote. The only change we will make is replacing the Sigmoid layer with a Tanh layer. Tanh outputs are always between -1 and 1, so that makes more sense given the data labels."
      ],
      "metadata": {
        "id": "v73BawFa2Mrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionPredictor(nn.Module):\n",
        "    def __init__(self, vocabulary_size: int, embedding_dimension: int):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_dimension)\n",
        "        self.linear_layer = nn.Linear(embedding_dimension, 1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.embedding_layer(x)\n",
        "        averaged = torch.mean(embeddings, axis = 1)\n",
        "        projected = self.linear_layer(averaged)\n",
        "        return self.tanh(projected)"
      ],
      "metadata": {
        "id": "OP0c76A32WPX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we train the model using our standard training loop. One difference you will notice is that I choose a random 64 sized subset of the total dataset (thousands and thousands of examples), which speeds up training."
      ],
      "metadata": {
        "id": "-u6u5V3I5ZyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dimension = 256\n",
        "model = EmotionPredictor(vocab_size, embedding_dimension)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "for i in range(1000):\n",
        "  randperm = torch.randperm(len(training_dataset))\n",
        "  training_dataset, training_labels = training_dataset[randperm], training_labels[randperm]\n",
        "  mini_batch = training_dataset[:64]\n",
        "  mini_batch_labels = training_labels[:64]\n",
        "  pred = model(mini_batch)\n",
        "  optimizer.zero_grad()\n",
        "  loss = loss_function(pred, mini_batch_labels)\n",
        "  if i % 100 == 0:\n",
        "    print(loss.item())\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "KDEStrPL2vxZ",
        "outputId": "43aa6a0c-5856-41f7-81a1-fb02665f0016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3156951069831848\n",
            "0.10798270255327225\n",
            "0.10898574441671371\n",
            "0.1617296189069748\n",
            "0.11518245935440063\n",
            "0.14829161763191223\n",
            "0.1892046332359314\n",
            "0.13579609990119934\n",
            "0.05430272966623306\n",
            "0.11518769711256027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see the model's outputs on some examples it's never seen before!\n",
        "\n"
      ],
      "metadata": {
        "id": "4H6OimX05pJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_one = \"worst movie ever\"\n",
        "\n",
        "example_two = \"best movie ever\"\n",
        "\n",
        "example_three = \"weird but funny movie\"\n",
        "\n",
        "examples = [example_one] + [example_two] + [example_three]\n",
        "\n",
        "# Let's encode these strings as numbers using the dictionary from earlier\n",
        "var_len = []\n",
        "for example in examples:\n",
        "  int_version = []\n",
        "  for word in example.split():\n",
        "    int_version.append(word_to_int[word])\n",
        "  var_len.append(torch.tensor(int_version))\n",
        "\n",
        "testing_tensor = torch.nn.utils.rnn.pad_sequence(var_len, batch_first=True)\n",
        "model.eval()\n",
        "\n",
        "print(model(testing_tensor).tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYW_lHXz5r8I",
        "outputId": "fcc8c97f-2f6e-4aeb-f2c8-98c4997e687a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.9936712384223938], [0.9998611211776733], [0.7425278425216675]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should find that the model outputs something close to -1 for example one, something very close to 1 for example two, and something close to 0 for example three (neutral)!\n",
        "\n",
        "This was a very simple model, and we will build more complex ones in the next problems!"
      ],
      "metadata": {
        "id": "sSCZ-Z2T9bO5"
      }
    }
  ]
}