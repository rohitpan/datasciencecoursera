{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORRBIq/id86slfSZEOpHrP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitpan/datasciencecoursera/blob/master/Embedding_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPICUIScuc-A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vec : semanticaly similar words will be near in the vector space.\n",
        "Word embedding : captures semantic meaning of the word.\n",
        "Senctence Embedding : captures the sementic meaning fo the sentence.\n",
        "\n",
        "WORD2VEC -> Semantic Word Embedding\n",
        "BERT -> Contexulized Word Embedding\n",
        "\n",
        "word2vec give back word_vectors (word to vector map)\n",
        "BERT using HF transformer library load the tokenizer and model. given sentence first tokenize it using tokenizer then pass the token to the model output token will be [CLS] word tokens\n",
        "\n",
        "BERT 12 Layers, 30K token size, 768 embed dimension\n",
        "\n",
        "SENTENCE EMBEDDING\n",
        "Mean Pooling of the contexual embedding vector at the last layer of BERT.\n",
        "Problem : consine similarity between two mean pool sentence embedding for semantically different sentence gives very high value.\n",
        "Solution : USE architecture to generate sentence embedding give correct sentence embedding.\n",
        "\n",
        "USE : Has deep averaging network for sentence embedding\n",
        "\n",
        "DUAL Encoder Architecute for Question Answer Retreival:\n",
        "Use two seperator Transformer based Encoder and Decoder model and user contrastive_loss( Using CrossEntropyLoss trick) to train the models.\n",
        "\n"
      ],
      "metadata": {
        "id": "WKOP7p2zud6F"
      }
    }
  ]
}