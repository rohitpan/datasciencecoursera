{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmdxM+19929l46q5O7PrO6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitpan/datasciencecoursera/blob/master/Embedding_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPICUIScuc-A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vec : semanticaly similar words will be near in the vector space.\n",
        "Word embedding : captures semantic meaning of the word.\n",
        "Senctence Embedding : captures the sementic meaning fo the sentence.\n",
        "\n",
        "WORD2VEC -> Semantic Word Embedding\n",
        "BERT -> Contexulized Word Embedding\n",
        "\n",
        "word2vec give back word_vectors (word to vector map)\n",
        "BERT using HF transformer library load the tokenizer and model. given sentence first tokenize it using tokenizer then pass the token to the model output token will be [CLS] word tokens [SEP]"
      ],
      "metadata": {
        "id": "WKOP7p2zud6F"
      }
    }
  ]
}