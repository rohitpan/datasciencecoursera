{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2Rz6zDL7aaZO9wJGGip2B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitpan/datasciencecoursera/blob/master/DGN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DGN using ngram tfidf feature vector then reduced to 100 dim and using RandomForest\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_csv('./dga_data.csv')  # Replace with the correct path to your CSV file\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "df = df.dropna(subset=['domain'])  # Remove rows where 'domain' is NaN\n",
        "df['isDGA'] = df['isDGA'].apply(lambda x: 1 if x == 'dga' else 0)  # Convert 'dga' to 1 and 'legit' to 0\n",
        "\n",
        "# Step 3: Generate TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=5000)  # Limit to 5000 features\n",
        "X = tfidf_vectorizer.fit_transform(df['domain'])\n",
        "y = df['isDGA']\n",
        "\n",
        "# Optional: Dimensionality Reduction\n",
        "svd = TruncatedSVD(n_components=100, random_state=42)\n",
        "X = svd.fit_transform(X)\n",
        "\n",
        "# Step 4: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train a Machine Learning Model\n",
        "model = RandomForestClassifier(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluate the Model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_TijnK8QTy8",
        "outputId": "a8d7a825-0192-4ca4-9bc8-8b62c86c4869"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.81603125\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.79      0.81     16123\n",
            "           1       0.80      0.84      0.82     15877\n",
            "\n",
            "    accuracy                           0.82     32000\n",
            "   macro avg       0.82      0.82      0.82     32000\n",
            "weighted avg       0.82      0.82      0.82     32000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# NN with feature enginnering\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_csv('./dga_data.csv')  # Replace with the correct path to your CSV file\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "data = df.dropna(subset=['domain'])  # Remove rows where 'domain' is NaN\n",
        "data['isDGA'] = data['isDGA'].apply(lambda x: 1 if x == 'dga' else 0)  # Convert 'dga' to 1 and 'legit' to 0\n",
        "\n",
        "\n",
        "\n",
        "# Feature Engineering Functions\n",
        "def calculate_entropy(domain):\n",
        "    prob = [float(domain.count(c)) / len(domain) for c in dict.fromkeys(list(domain))]\n",
        "    entropy = -sum([p * np.log2(p) for p in prob])\n",
        "    return entropy\n",
        "\n",
        "# Feature Engineering\n",
        "data['domain_length'] = data['domain'].apply(len)\n",
        "data['vowel_ratio'] = data['domain'].apply(lambda x: sum(1 for c in x if c in 'aeiouAEIOU') / len(x))\n",
        "data['consonant_ratio'] = data['domain'].apply(lambda x: sum(1 for c in x if c.isalpha() and c not in 'aeiouAEIOU') / len(x))\n",
        "data['digit_ratio'] = data['domain'].apply(lambda x: sum(1 for c in x if c.isdigit()) / len(x))\n",
        "data['hexadecimal_ratio'] = data['domain'].apply(lambda x: sum(1 for c in x if c in '0123456789abcdefABCDEF') / len(x))\n",
        "data['entropy'] = data['domain'].apply(calculate_entropy)\n",
        "#'isDGA', 'domain', 'host', 'subclass'\n",
        "# Ensure we're only using the numeric feature-engineered columns for training\n",
        "X = data.drop(columns=['domain', 'isDGA','host','subclass']).values\n",
        "y = data['isDGA'].values\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print out to debug what's inside X_train before conversion to tensor\n",
        "print(\"column names\",data.columns)\n",
        "print(\"X_train sample:\", X_train[:5])\n",
        "\n",
        "# Convert data to PyTorch tensors for wide path\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define Dataset for PyTorch\n",
        "class DGADataset(Dataset):\n",
        "    def __init__(self, X_wide, y):\n",
        "        self.X_wide = X_wide\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X_wide[idx], self.y[idx]\n",
        "\n",
        "train_dataset = DGADataset(X_train_tensor, y_train_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Define the PyTorch model (Wide Part Only)\n",
        "class WideModel(nn.Module):\n",
        "    def __init__(self, wide_features_dim):\n",
        "        super(WideModel, self).__init__()\n",
        "\n",
        "        # Dense layer for the wide path\n",
        "        self.wide_layer = nn.Linear(wide_features_dim, 64)\n",
        "\n",
        "        # Dense output layer\n",
        "        self.output_layer = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, X_wide):\n",
        "        # Wide path\n",
        "        X_wide = self.wide_layer(X_wide)\n",
        "        X_wide = F.relu(X_wide)  # Apply ReLU activation\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(X_wide)\n",
        "        return torch.sigmoid(output)\n",
        "\n",
        "# Hyperparameters\n",
        "wide_features_dim = X_train_tensor.shape[1]\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = WideModel(wide_features_dim)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_wide_batch, y_batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_wide_batch).squeeze()\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_dataloader)}')\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Step 6: Evaluate the Model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor).squeeze()\n",
        "    y_pred = (y_pred_tensor > 0.5).float().numpy()  # Convert to binary 0/1 predictions\n",
        "\n",
        "# Calculate accuracy and classification report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Yphx7yeTTMAn",
        "outputId": "0b70c1d7-5b30-45db-8bc4-1ca597366760"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-99fa521f9359>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['isDGA'] = data['isDGA'].apply(lambda x: 1 if x == 'dga' else 0)  # Convert 'dga' to 1 and 'legit' to 0\n",
            "<ipython-input-8-99fa521f9359>:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['domain_length'] = data['domain'].apply(len)\n",
            "<ipython-input-8-99fa521f9359>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['vowel_ratio'] = data['domain'].apply(lambda x: sum(1 for c in x if c in 'aeiouAEIOU') / len(x))\n",
            "<ipython-input-8-99fa521f9359>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['consonant_ratio'] = data['domain'].apply(lambda x: sum(1 for c in x if c.isalpha() and c not in 'aeiouAEIOU') / len(x))\n",
            "<ipython-input-8-99fa521f9359>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['digit_ratio'] = data['domain'].apply(lambda x: sum(1 for c in x if c.isdigit()) / len(x))\n",
            "<ipython-input-8-99fa521f9359>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['hexadecimal_ratio'] = data['domain'].apply(lambda x: sum(1 for c in x if c in '0123456789abcdefABCDEF') / len(x))\n",
            "<ipython-input-8-99fa521f9359>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['entropy'] = data['domain'].apply(calculate_entropy)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "column names Index(['isDGA', 'domain', 'host', 'subclass', 'domain_length', 'vowel_ratio',\n",
            "       'consonant_ratio', 'digit_ratio', 'hexadecimal_ratio', 'entropy'],\n",
            "      dtype='object')\n",
            "X_train sample: [[15.          0.33333333  0.66666667  0.          0.4         3.32323143]\n",
            " [ 5.          0.2         0.8         0.          0.4         2.32192809]\n",
            " [15.          0.4         0.6         0.          0.33333333  3.1068906 ]\n",
            " [18.          0.16666667  0.83333333  0.          0.11111111  3.68354236]\n",
            " [ 9.          0.          1.          0.          0.11111111  2.72548056]]\n",
            "Epoch 1/5, Loss: 0.33524478199561014\n",
            "Epoch 2/5, Loss: 0.30180840441679607\n",
            "Epoch 3/5, Loss: 0.2960024307337315\n",
            "Epoch 4/5, Loss: 0.2936482599378509\n",
            "Epoch 5/5, Loss: 0.2924516286178885\n",
            "Training complete!\n",
            "Accuracy: 0.87065625\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.97      0.88     16123\n",
            "           1       0.96      0.77      0.86     15877\n",
            "\n",
            "    accuracy                           0.87     32000\n",
            "   macro avg       0.89      0.87      0.87     32000\n",
            "weighted avg       0.89      0.87      0.87     32000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_csv('./dga_data.csv')  # Replace with the correct path to your CSV file\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "data = df.dropna(subset=['domain'])  # Remove rows where 'domain' is NaN\n",
        "data['isDGA'] = data['isDGA'].apply(lambda x: 1 if x == 'dga' else 0)  # Convert 'dga' to 1 and 'legit' to 0\n",
        "\n",
        "def calculate_entropy(domain):\n",
        "    prob = [float(domain.count(c)) / len(domain) for c in dict.fromkeys(list(domain))]\n",
        "    entropy = -sum([p * np.log2(p) for p in prob])\n",
        "    return entropy\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "data['domain_length'] = data['domain'].apply(len)\n",
        "data['vowel_ratio'] = data['domain'].apply(lambda x: sum(1 for c in x if c in 'aeiouAEIOU') / len(x))\n",
        "data['consonant_ratio'] = data['domain'].apply(lambda x: sum(1 for c in x if c.isalpha() and c not in 'aeiouAEIOU') / len(x))\n",
        "data['digit_ratio'] = data['domain'].apply(lambda x: sum(1 for c in x if c.isdigit()) / len(x))\n",
        "data['hexadecimal_ratio'] = data['domain'].apply(lambda x: sum(1 for c in x if c in '0123456789abcdefABCDEF') / len(x))\n",
        "data['entropy'] = data['domain'].apply(calculate_entropy)\n",
        "\n",
        "# Step 4: Prepare data for PyTorch\n",
        "X_wide = data.drop(columns=['domain', 'isDGA', 'host', 'subclass']).values\n",
        "X_deep = data['domain'].values  # Handle domain strings for the deep part\n",
        "y = data['isDGA'].values\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_wide_train, X_wide_test, X_deep_train, X_deep_test, y_train, y_test = train_test_split(\n",
        "    X_wide, X_deep, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert X_wide and y into PyTorch tensors\n",
        "X_wide_train_tensor = torch.tensor(X_wide_train, dtype=torch.float32)\n",
        "X_wide_test_tensor = torch.tensor(X_wide_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Step 5: Define Dataset class for PyTorch\n",
        "class DGADataset(Dataset):\n",
        "    def __init__(self, X_wide, X_deep, y):\n",
        "        self.X_wide = X_wide\n",
        "        self.X_deep = X_deep\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert X_deep (domain names) into tensors of ordinal values\n",
        "        X_deep_tensor = torch.tensor([ord(c) for c in self.X_deep[idx]], dtype=torch.long)\n",
        "        return self.X_wide[idx], X_deep_tensor, self.y[idx]\n",
        "\n",
        "# Step 6: Custom collate_fn to pad sequences in X_deep_batch\n",
        "def collate_fn(batch):\n",
        "    X_wide_batch, X_deep_batch, y_batch = zip(*batch)\n",
        "\n",
        "    # Convert X_wide_batch to tensor using torch.stack\n",
        "    X_wide_batch = torch.stack([torch.tensor(x_wide, dtype=torch.float32) for x_wide in X_wide_batch])\n",
        "\n",
        "    # Convert y_batch to tensor\n",
        "    y_batch = torch.tensor(y_batch, dtype=torch.float32)\n",
        "\n",
        "    # Pad X_deep_batch sequences to the maximum length in this batch\n",
        "    X_deep_batch_padded = nn.utils.rnn.pad_sequence(X_deep_batch, batch_first=True, padding_value=0)\n",
        "\n",
        "    return X_wide_batch, X_deep_batch_padded, y_batch\n",
        "\n",
        "# Create train dataset and dataloader with custom collate_fn\n",
        "train_dataset = DGADataset(X_wide_train_tensor, X_deep_train, y_train_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Define the PyTorch model\n",
        "class WideDeepModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, wide_features_dim):\n",
        "        super(WideDeepModel, self).__init__()\n",
        "\n",
        "        # Embedding layer for the deep path\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # LSTM layer for the deep path\n",
        "        self.lstm = nn.LSTM(embed_dim, 64, batch_first=True)\n",
        "\n",
        "        # Dense layer for the wide path\n",
        "        self.wide_layer = nn.Linear(wide_features_dim, 64)\n",
        "\n",
        "        # Dense output layer\n",
        "        self.output_layer = nn.Linear(64 + 64, 1)\n",
        "\n",
        "    def forward(self, X_wide, X_deep):\n",
        "        # Deep path\n",
        "        X_deep = self.embedding(X_deep)\n",
        "        X_deep, _ = self.lstm(X_deep)\n",
        "        X_deep = X_deep[:, -1, :]  # Use last output of LSTM\n",
        "\n",
        "        # Wide path\n",
        "        X_wide = self.wide_layer(X_wide)\n",
        "\n",
        "        # Combine wide and deep paths\n",
        "        X_combined = torch.cat((X_wide, X_deep), dim=1)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(X_combined)\n",
        "        return torch.sigmoid(output)\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = 256  # Assuming ASCII characters\n",
        "embed_dim = 128\n",
        "wide_features_dim = X_wide_train_tensor.shape[1]\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideDeepModel(vocab_size, embed_dim, wide_features_dim).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 7: Training loop\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_wide_batch, X_deep_batch, y_batch in train_dataloader:\n",
        "        X_wide_batch, X_deep_batch, y_batch = X_wide_batch.to(device), X_deep_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_wide_batch, X_deep_batch).squeeze()\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_dataloader)}')\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Step 8: Evaluate the Model\n",
        "test_dataset = DGADataset(X_wide_test_tensor, X_deep_test, y_test_tensor)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_list = []\n",
        "    y_test_list = []\n",
        "    for X_wide_batch, X_deep_batch, y_batch in test_dataloader:\n",
        "        X_wide_batch, X_deep_batch = X_wide_batch.to(device), X_deep_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        outputs = model(X_wide_batch, X_deep_batch).squeeze()\n",
        "        y_pred = (outputs > 0.5).float()\n",
        "        y_pred_list.append(y_pred.cpu().numpy())\n",
        "        y_test_list.append(y_batch.cpu().numpy())\n",
        "\n",
        "# Convert prediction lists to numpy arrays for accuracy computation\n",
        "y_pred_final = np.concatenate(y_pred_list)\n",
        "y_test_final = np.concatenate(y_test_list)\n",
        "\n",
        "# Step 9: Calculate accuracy and classification report\n",
        "print(\"Accuracy:\", accuracy_score(y_test_final, y_pred_final))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_final, y_pred_final))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhexRwN2pSkU",
        "outputId": "3c06f991-2d69-40de-b2de-6caab9890bba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b01ab3c26ede>:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_wide_batch = torch.stack([torch.tensor(x_wide, dtype=torch.float32) for x_wide in X_wide_batch])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.004032328282957473\n",
            "Epoch 2/20, Loss: 3.7828380246698065e-05\n",
            "Epoch 3/20, Loss: 1.3945251012834773e-05\n",
            "Epoch 4/20, Loss: 6.679219688119981e-06\n",
            "Epoch 5/20, Loss: 3.4957971595376855e-06\n",
            "Epoch 6/20, Loss: 1.9066103509576313e-06\n",
            "Epoch 7/20, Loss: 1.1010871380561713e-06\n",
            "Epoch 8/20, Loss: 6.500708318230793e-07\n",
            "Epoch 9/20, Loss: 3.905886675099711e-07\n",
            "Epoch 10/20, Loss: 2.3148660296913648e-07\n",
            "Epoch 11/20, Loss: 1.35422065459644e-07\n",
            "Epoch 12/20, Loss: 8.012673984834656e-08\n",
            "Epoch 13/20, Loss: 4.3150762523747905e-08\n",
            "Epoch 14/20, Loss: 2.701238567352239e-08\n",
            "Epoch 15/20, Loss: 1.79582306846466e-08\n",
            "Epoch 16/20, Loss: 1.061529274160045e-08\n",
            "Epoch 17/20, Loss: 5.824371664015516e-09\n",
            "Epoch 18/20, Loss: 2.2420378756163807e-09\n",
            "Epoch 19/20, Loss: 1.0418514403451722e-09\n",
            "Epoch 20/20, Loss: 8.573168442790183e-10\n",
            "Training complete!\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       1.00      1.00      1.00      7751\n",
            "\n",
            "    accuracy                           1.00      7751\n",
            "   macro avg       1.00      1.00      1.00      7751\n",
            "weighted avg       1.00      1.00      1.00      7751\n",
            "\n"
          ]
        }
      ]
    }
  ]
}